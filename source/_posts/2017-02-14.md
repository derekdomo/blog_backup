---
title: Memorizing the data
date: 2017-02-14 22:40:05
tags: [新知识]
categories: [DataScience]
---
最近读了一篇paper，Understanding deep learning requires rethinking Generalization。简直是现在DL热潮中一股清流。

# Memorizing the data
* 我对这篇文章印象最深的一句话：The effective capacity of neural networks is large enough for a brute-force memorization of the entire data set. 
* 他们做了几个实验，就是把训练数据的label全部打乱，然后在这基础上训练几个不同类型的神经网络，结果发现都可以达到zero training error。这就不得不让人觉得神经网络知识记住这些data而已，并没有从中总结出规律。
* 闲扯一下，真的神经网络不就是这样子么，记录东西，下次有相似的事情发生时，可以知道如何应对，这就是学习的过程啊。只不过现在感觉主要的工业界需求在于机器学习带来的insights，得到数据中隐藏的规律。

# Regularization
regularization运用在模型中，可以提升模型的generalization，避免overfitting。我理解中的regularizer的确是这样子，但是这个paper说到，finely tuned parameters of regularizer can improve generalization, but it is not the fundamental reasons of generalization。因为他们在实验中发现去掉regularizer，神经网络也可以表现得很好。。。
提到regularization，一共有以下几种常用的：

* Data augmentation
    * augment dataset according to some domain specific knowledge
* Weight Decay
    * something like l2 regularizer
* Dropout
    * mask out each element of a layer output randomly with a given dropout probability.

# Random thoughts
最近总有一些很奇怪的想法，如果神经网络是在memorize data，那这个能用来压缩数据么？如果一些数据的目的性十分明确，那何尝不是一个方法，训练完就可以丢掉，需要存储的存储的只是几个parameter而已，只是问题是怎么把新数据加进来，同时又不会造成之前的数据信息丢失。
Anyway，总的来说这篇文章还是挺有意思的，因为当下deep learning实在是太火，但是很难去理解这个模型，工业界的运用有时候不仅仅是正确率是唯一metrics，更多的是需要模型来教给人一些东西，来提供更多的insights。这个从generalization的角度去分析现在的神经网络的局限性，提出这个模型只是单纯的在记录数据。

